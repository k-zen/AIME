<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>http.agent.name</name>
        <value></value>
        <description>
            HTTP 'User-Agent' request header. i.e. AIME: Aimebot/0.2 (+http://aime.io/robot; aimebot@aime.io)
        </description>
    </property>
    <property>
        <name>http.robots.agents</name>
        <value>*</value>
        <description>
            The agent strings we'll look for in robots.txt files, comma-separated, in decreasing order of precedence. You should
            put the value of http.agent.name as the first agent name, and keep the default * at the end of the list. E.g.: BlurflDev,Blurfl,*
        </description>
    </property>
    <property>
        <name>http.robots.403.allow</name>
        <value>true</value>
        <description>
            Some servers return HTTP status 403 (Forbidden) if /robots.txt doesn't exist. This should probably mean that we are
            allowed to crawl the site nonetheless. If this is set to false, then such sites will be treated as forbidden.
        </description>
    </property>
    <property>
        <name>http.agent.host</name>
        <value></value>
        <description>
            Name or IP address of the host on which the AIME crawler would be running. Currently this is used by 'protocol-httpclient' plugin.
        </description>
    </property>
    <property>
        <name>http.timeout</name>
        <value>10000</value>
        <description>
            The default network timeout, in milliseconds.
        </description>
    </property>
    <property>
        <name>http.max.delays</name>
        <value>100</value>
        <description>
            The number of times a thread will delay when trying to fetch a page.  Each time it finds that a host is busy, it will wait
            fetcher.server.delay. After http.max.delays attepts, it will give up on the page for now.
        </description>
    </property>
    <property>
        <name>http.content.limit</name>
        <value>65536</value>
        <description>
            The length limit for downloaded content using the http protocol, in bytes. If this value is nonnegative (>=0), content longer
            than it will be truncated; otherwise, no truncation at all. Do not confuse this setting with the file.content.limit setting.
        </description>
    </property>
    <property>
        <name>http.proxy.host</name>
        <value></value>
        <description>
            The proxy hostname. If empty, no proxy is used.
        </description>
    </property>
    <property>
        <name>http.proxy.port</name>
        <value></value>
        <description>
            The proxy port.
        </description>
    </property>
    <property>
        <name>http.proxy.username</name>
        <value></value>
        <description>
            Username for proxy. This will be used by 'protocol-httpclient', if the proxy server requests basic, digest and/or NTLM 
            authentication. To use this, 'protocol-httpclient' must be present in the value of 'plugin.includes' property.
            NOTE: For NTLM authentication, do not prefix the username with the domain, i.e. 'susam' is correct whereas 'DOMAIN\susam' is 
            incorrect.
        </description>
    </property>
    <property>
        <name>http.proxy.password</name>
        <value></value>
        <description>
            Password for proxy. This will be used by 'protocol-httpclient', if the proxy server requests basic, digest and/or 
            NTLM authentication. To use this, 'protocol-httpclient' must be present in the value of 'plugin.includes' property.
        </description>
    </property>
    <property>
        <name>http.proxy.realm</name>
        <value></value>
        <description>
            Authentication realm for proxy. Do not define a value if realm is not required or authentication should take place for any
            realm. NTLM does not use the notion of realms. Specify the domain name of NTLM authentication as the value for this 
            property. To use this, 'protocol-httpclient' must be present in the value of 'plugin.includes' property.
        </description>
    </property>
    <property>
        <name>http.auth.file</name>
        <value>httpclient-auth.xml</value>
        <description>
            Authentication configuration file for 'protocol-httpclient' plugin.
        </description>
    </property>
    <property>
        <name>http.verbose</name>
        <value>false</value>
        <description>
            If true, HTTP will log more verbosely.
        </description>
    </property>
    <property>
        <name>http.redirect.max</name>
        <value>0</value>
        <description>
            The maximum number of redirects the fetcher will follow when trying to fetch a page. If set to negative or 0, 
            fetcher won't immediately follow redirected URLs, instead it will record them for later fetching.
        </description>
    </property>
    <property>
        <name>http.useHttp11</name>
        <value>false</value>
        <description>
            NOTE: at the moment this works only for protocol-httpclient. If true, use HTTP 1.1, if false use HTTP 1.0 .
        </description>
    </property>
    <property>
        <name>http.accept.language</name>
        <value>en-us,en-gb,en;q=0.7,*;q=0.3</value>
        <description>
            Value of the "Accept-Language" request header field. This allows selecting non-English language as default one to retrieve.
            It is a useful setting for search engines build for certain national group.
        </description>
    </property>
    <property>
        <name>db.default.fetch.interval</name>
        <value>30</value>
        <description>
            (DEPRECATED) The default number of days between re-fetches of a page.
        </description>
    </property>
    <property>
        <name>db.fetch.interval.default</name>
        <value>2592000</value>
        <description>
            The default number of seconds between re-fetches of a page (30 days).
        </description>
    </property>
    <property>
        <name>db.fetch.interval.max</name>
        <value>7776000</value>
        <description>
            The maximum number of seconds between re-fetches of a page (90 days). After this period every page in the db will be re-tried, no
            matter what is its status.
        </description>
    </property>
    <property>
        <name>db.fetch.schedule.class</name>
        <value>io.aime.crawl.DefaultFetchSchedule</value>
        <description>
            The implementation of fetch schedule. "DefaultFetchSchedule" simply adds the original fetchInterval to the last fetch time, 
            regardless of page changes.
        </description>
    </property>
    <property>
        <name>db.fetch.schedule.adaptive.inc_rate</name>
        <value>0.4</value>
        <description>
            If a page is unmodified, its fetchInterval will be increased by this rate. This value should not exceed 0.5, otherwise the 
            algorithm becomes unstable.
        </description>
    </property>
    <property>
        <name>db.fetch.schedule.adaptive.dec_rate</name>
        <value>0.2</value>
        <description>
            If a page is modified, its fetchInterval will be decreased by this rate. This value should not exceed 0.5, otherwise 
            the algorithm becomes unstable.
        </description>
    </property>
    <property>
        <name>db.fetch.schedule.adaptive.min_interval</name>
        <value>60.0</value>
        <description>
            Minimum fetchInterval, in seconds.
        </description>
    </property>
    <property>
        <name>db.fetch.schedule.adaptive.max_interval</name>
        <value>31536000.0</value>
        <description>
            Maximum fetchInterval, in seconds (365 days). NOTE: this is limited by db.fetch.interval.max. Pages with fetchInterval 
            larger than db.fetch.interval.max will be fetched anyway.
        </description>
    </property>
    <property>
        <name>db.fetch.schedule.adaptive.sync_delta</name>
        <value>true</value>
        <description>
            If true, try to synchronize with the time of page change by shifting the next fetchTime by a fraction (sync_rate) of the difference
            between the last modification time, and the last fetch time.
        </description>
    </property>
    <property>
        <name>db.fetch.schedule.adaptive.sync_delta_rate</name>
        <value>0.3</value>
        <description>
            See sync_delta for description. This value should not exceed 0.5, otherwise the algorithm becomes unstable.
        </description>
    </property>
    <property>
        <name>db.update.additions.allowed</name>
        <value>true</value>
        <description>
            If true, updatedb will add newly discovered URLs, if false only already existing URLs in the CrawlDb will be updated and no new
            URLs will be added.
        </description>
    </property>
    <property>
        <name>db.update.max.inlinks</name>
        <value>10000</value>
        <description>
            Maximum number of inlinks to take into account when updating a URL score in the crawlDB. Only the best scoring inlinks are kept.
        </description>
    </property>
    <property>
        <name>db.ignore.internal.links</name>
        <value>true</value>
        <description>
            If true, when adding new links to a page, links from the same host are ignored.  This is an effective way to limit the
            size of the link database, keeping only the highest quality links.
        </description>
    </property>
    <property>
        <name>db.ignore.external.links</name>
        <value>false</value>
        <description>
            If true, outlinks leading from a page to external hosts will be ignored. This is an effective way to limit the crawl to include
            only initially injected hosts, without creating complex URLFilters.
        </description>
    </property>
    <property>
        <name>db.score.injected</name>
        <value>1.0</value>
        <description>
            The score of new pages added by the injector.
        </description>
    </property>
    <property>
        <name>db.max.outlinks.per.page</name>
        <value>100</value>
        <description>
            The maximum number of outlinks that we'll process for a page. If this value is nonnegative (>=0), at most 
            db.max.outlinks.per.page outlinks will be processed for a page; otherwise, all outlinks will be processed.
        </description>
    </property>
    <property>
        <name>db.parsemeta.to.crawldb</name>
        <value></value>
        <description>
            Comma-separated list of parse metadata keys to transfer to the crawldb (NUTCH-779). Assuming for instance that the 
            languageidentifier plugin is enabled, setting the value to 'lang' will copy both the key 'lang' and its value to the 
            corresponding entry in the crawldb.
        </description>
    </property>
    <property>
        <name>db.fetch.retry.max</name>
        <value>3</value>
        <description>
            The maximum number of times a url that has encountered recoverable errors is generated for fetch.
        </description>
    </property>
    <property>
        <name>generate.max.count</name>
        <value>-1</value>
        <description>
            The maximum number of urls in a single fetchlist. -1 if unlimited. The urls are counted according to the value of the 
            parameter generator.count.mode.
        </description>
    </property>
    <property>
        <name>generate.count.mode</name>
        <value>host</value>
        <description>
            Determines how the URLs are counted for generator.max.count. Default value is 'host' but can be 'domain'. Note that we do not count
            per IP in the new version of the Generator.
        </description>
    </property>
    <property>
        <name>generate.update.crawldb</name>
        <value>false</value>
        <description>
            For highly-concurrent environments, where several generate/fetch/update cycles may overlap, setting this to true ensures
            that generate will create different fetchlists even without intervening updatedb-s, at the cost of running an additional job to 
            update CrawlDB. If false, running generate twice without intervening updatedb will generate identical fetchlists.
        </description>
    </property>
    <property>
        <name>generate.max.per.host</name>
        <value>-1</value>
        <description>
            (Deprecated). Use generate.max.count and generate.count.mode instead. The maximum number of urls per host in a single
            fetchlist. -1 if unlimited.
        </description>
    </property>
    <property>
        <name>partition.url.mode</name>
        <value>byHost</value>
        <description>
            Determines how to partition URLs. Default value is 'byHost', also takes 'byDomain' or 'byIP'.
        </description>
    </property>
    <property>
        <name>crawl.gen.delay</name>
        <value>604800000</value>
        <description>
            This value, expressed in days, defines how long we should keep the lock on records in CrawlDb that were just selected 
            for fetching. If these records are not updated in the meantime, the lock is canceled, i.e. the become eligible for selecting.
            Default value of this is 7 days.
        </description>
    </property>
    <property>
        <name>fetcher.server.delay</name>
        <value>5.0</value>
        <description>
            The number of seconds the fetcher will delay between successive requests to the same server.
        </description>
    </property>
    <property>
        <name>fetcher.server.min.delay</name>
        <value>0.0</value>
        <description>
            The minimum number of seconds the fetcher will delay between successive requests to the same server. This value is applicable ONLY
            if fetcher.threads.per.host is greater than 1 (i.e. the host blocking is turned off).
        </description>
    </property>
    <property>
        <name>fetcher.max.crawl.delay</name>
        <value>30</value>
        <description>
            If the Crawl-Delay in robots.txt is set to greater than this value (in seconds) then the fetcher will skip this page, 
            generating an error report. If set to -1 the fetcher will never skip such pages and will wait the
            amount of time retrieved from robots.txt Crawl-Delay, however long that might be.
        </description>
    </property>
    <property>
        <name>fetcher.threads.fetch</name>
        <value>10</value>
        <description>
            The number of FetcherThreads the fetcher should use. This is also determines the maximum number of requests that are
            made at once (each FetcherThread handles one connection).
        </description>
    </property>
    <property>
        <name>fetcher.threads.per.host</name>
        <value>1</value>
        <description>
            This number is the maximum number of threads that should be allowed to access a host at one time.
        </description>
    </property>
    <property>
        <name>fetcher.threads.per.host.by.ip</name>
        <value>true</value>
        <description>
            If true, then fetcher will count threads by IP address, to which the URL's host name resolves. If false, only host name will be
            used. NOTE: this should be set to the same value as "generate.max.per.host.by.ip" - default settings are different only for
            reasons of backward-compatibility.
        </description>
    </property>
    <property>
        <name>fetcher.verbose</name>
        <value>false</value>
        <description>
            If true, fetcher will log more verbosely.
        </description>
    </property>
    <property>
        <name>fetcher.parse</name>
        <value>true</value>
        <description>
            If true, fetcher will parse content.
        </description>
    </property>
    <property>
        <name>fetcher.store.content</name>
        <value>true</value>
        <description>
            If true, fetcher will store content.
        </description>
    </property>
    <property>
        <name>fetcher.timelimit.mins</name>
        <value>-1</value>
        <description>
            This is the number of minutes allocated to the fetching. Once this value is reached, any remaining entry from the input 
            URL list is skipped and all active queues are emptied. The default value of -1 deactivates the time limit.
        </description>
    </property>
    <property>
        <name>fetcher.max.exceptions.per.queue</name>
        <value>-1</value>
        <description>
            The maximum number of protocol-level exceptions (e.g. timeouts) per host (or IP) queue. Once this value is reached, 
            any remaining entries from this queue are purged, effectively stopping the fetching from this host/IP. The default
            value of -1 deactivates this limit.
        </description>
    </property>
    <property>
        <name>indexingfilter.order</name>
        <value></value>
        <description>
            The order by which index filters are applied. If empty, all available index filters (as dictated by properties
            plugin-includes and plugin-excludes above) are loaded and applied in system defined order. If not empty, only named 
            filters are loaded and applied in given order.
        </description>
    </property>
    <property>
        <name>urlnormalizer.order</name>
        <value></value>
        <description>
            Order in which normalizers will run. If any of these isn't activated it will be silently skipped. If other normalizers not on the
            list are activated, they will run in random order after the ones specified here are run.
        </description>
    </property>
    <property>
        <name>urlnormalizer.regex.file</name>
        <value>regex-normalize.xml</value>
        <description>
            Name of the config file used by the RegexUrlNormalizer class.
        </description>
    </property>
    <property>
        <name>urlnormalizer.loop.count</name>
        <value>1</value>
        <description>
            Optionally loop through normalizers several times, to make sure that all transformations have been performed.
        </description>
    </property>
    <property>
        <name>mime.type.magic</name>
        <value>true</value>
        <description>
            Defines if the mime content type detector uses magic resolution.
        </description>
    </property>
    <property>
        <name>plugin.folders</name>
        <value>plugins</value>
        <description>
            Directories where AIME plugins are located. Each element may be a relative or absolute path. If absolute, it is used
            as is. If relative, it is searched for on the classpath.
        </description>
    </property>
    <property>
        <name>plugin.auto-activation</name>
        <value>true</value>
        <description>
            Defines if some plugins that are not activated regarding the plugin.includes and plugin.excludes properties must be automaticaly
            activated if they are needed by some actived plugins.
        </description>
    </property>
    <property>
        <name>plugin.includes</name>
        <value>language-identifier|protocol-(file|httpclient|smb)|urlfilter-regex|parse-(html|tika)|index-(basic|more)|urlnormalizer-(basic)</value>
        <description>
            Regular expression naming plugin directory names to include. Any plugin not matching this expression is excluded.
            In any case you need at least include the aime-extensionpoints plugin. By default AIME includes crawling just HTML and 
            plain text via HTTP, and basic indexing and search plugins. In order to use HTTPS please enable
            protocol-httpclient, but be aware of possible intermittent problems with the underlying commons-httpclient library. 
            Nutch now also includes integration with Tika to leverage Tika's parsing capabilities for multiple content types. The existing Nutch
            parser implementations will likely be phased out in the next release or so, as such, it is a good idea to begin migrating away 
            from anything not provided by parse-tika.
        </description>
    </property>
    <property>
        <name>plugin.excludes</name>
        <value></value>
        <description>
            Regular expression naming plugin directory names to exclude.
        </description>
    </property>
    <property>
        <name>parse.plugin.file</name>
        <value>parse-plugins.xml</value>
        <description>
            The name of the file that defines the associations between content-types and parsers.
        </description>
    </property>
    <property>
        <name>parser.character.encoding.default</name>
        <value>windows-1252</value>
        <description>
            The character encoding to fall back to when no other information is available
        </description>
    </property>
    <property>
        <name>encodingdetector.charset.min.confidence</name>
        <value>-1</value>
        <description>
            A integer between 0-100 indicating minimum confidence value for charset auto-detection. Any negative value disables auto-detection.
        </description>
    </property>
    <property>
        <name>parser.caching.forbidden.policy</name>
        <value>content</value>
        <description>
            If a site (or a page) requests through its robot metatags that it should not be shown as cached content, apply this policy. Currently
            three keywords are recognized: "none" ignores any "noarchive" directives. "content" doesn't show the content, but shows 
            summaries (snippets). "all" doesn't show either content or summaries.
        </description>
    </property>
    <property>
        <name>parser.html.form.use_action</name>
        <value>false</value>
        <description>
            If true, HTML parser will collect URLs from form action attributes. This may lead to undesirable behavior (submitting empty
            forms during next fetch cycle). If false, form action attribute will be ignored.
        </description>
    </property>
    <property>
        <name>parser.html.outlinks.ignore_tags</name>
        <value></value>
        <description>
            Comma separated list of HTML tags, from which outlinks shouldn't be extracted. AIME takes links from: a, area, form, frame,
            iframe, script, link, img. If you add any of those tags here, it won't be taken. Default is empty list. Probably reasonable value
            for most people would be "img,script,link".
        </description>
    </property>
    <property>
        <name>htmlparsefilter.order</name>
        <value></value>
        <description>
            The order by which HTMLParse filters are applied. If empty, all available HTMLParse filters (as dictated by properties
            plugin-includes and plugin-excludes above) are loaded and applied in system defined order. If not empty, only named filters are 
            loaded and applied in given order.
            HTMLParse filter ordering MAY have an impact on end result, as some filters could rely on the metadata generated by a previous filter.
        </description>
    </property>
    <property>
        <name>parser.timeout</name>
        <value>30</value>
        <description>
            Timeout in seconds for the parsing of a document, otherwise treats it as an exception and
            moves on the the following documents. This parameter is applied to any Parser implementation.
            Set to -1 to deactivate, bearing in mind that this could cause
            the parsing to crash because of a very long or corrupted document.
        </description>
    </property>
    <property>
        <name>urlfilter.order</name>
        <value></value>
        <description>
            The order by which url filters are applied. If empty, all available url filters (as dictated by properties
            plugin-includes and plugin-excludes above) are loaded and applied in system defined order.
            If not empty, only named filters are loaded and applied in given order. 
        </description>
    </property>
</configuration>